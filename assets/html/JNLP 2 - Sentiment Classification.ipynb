{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Journey into NLP 2 - Sentiment Classification\n",
    "### Haidar Khan - 3/14/2019\n",
    "\n",
    "## Introduction\n",
    "Welcome to the second article in this series on Natural Language Processing (NLP). As mentioned in the [first article](), the goal of this series is to develop an understanding of current approaches in NLP. In this article, we will discuss an approach to classifying text by the sentiment expressed in the text. The focus in this article will be understanding the problem setup and the code.\n",
    "\n",
    "In the first article, we discussed how words are represented in modern NLP systems. We covered two approaches to learning word representations, called embeddings. In this article we will use the word embeddings to train a machine learning model to classify short segments of text based on the sentiment expressed by the writer. The material from the first article is not required to follow this article. The only aspect of word embeddings that is important for this article is that the embeddings give us a way to represent words as low-dimensional, dense vectors that preserve low level relationships between words.\n",
    "\n",
    "## References\n",
    "The original papers for each method will be referenced whenever possible. \n",
    "\n",
    "The material in this series will be based mostly on the excellent set of lectures from Stanford University - Natural Language Processing taught by Christopher Manning and Richard Socher ([available on Youtube](https://www.youtube.com/watch?v=OQQ-W_63UgQ&list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6)) \n",
    "\n",
    "Code will also be referenced in each article. An excellent starting resource is [available here](https://github.com/lyeoni/nlp-tutorial)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification\n",
    "\n",
    "The sentiment classification problem is a subtask of the general text classification problem. We will describe the general text classification setup here and note that the setup for sentiment classification is identical.\n",
    "\n",
    "Text classification is an example of a supervised machine learning task. Supervised machine learning is when a model is shown examples that consist of two parts: the data and the label, and the model is asked to \"learn\" the relationship between the data and the labels during the training phase. When the model is deployed into the wild jungle that is the real world, the model is shown just the data and the expectation is that the label the model predicts corresponds to the true (and unknown) label. Thus, we see that there are two main phases for a machine learning model. In the training phase, the model updates its parameters to correctly match the data and labels of training examples. In the testing phase, the parameters of the model are fixed and the model simply outputs its best guess for the labels of the testing examples. For this type of setup, someone (usually many people) needs to exert many man-hours to correctly label the data.\n",
    "\n",
    "In the context of text classification, the data comprises of a sequence of words and the label represents some interesting classification of the sentence. For example, the label for a text could be \"declarative sentence\" indicating that the text is a special type of sentence called a declarative sentence. Another type of text label could be related to the usefullness of the text to other users, such as the usefullness score of a product review. For sentiment classification, the label corresponds to the feeling expressed in the review usually \"negative\", \"neutral\", or \"positive\".\n",
    "\n",
    "| Sentence ($\\mathbf{x}$) | Class ($y$) |\n",
    "| ----------------------------------------- | -------- |\n",
    "| A masterpiece four years in the making. | positive |\n",
    "| Offers a breath of the fresh air of true sophistication. | positive |\n",
    "| Light, cute and forgettable | negative |\n",
    "| Video games are more involving than this mess | negative |\n",
    "\n",
    "We will denote the sequence of words in a text as $\\mathbf{x}$ and the corresponding label as $y$. These pairs $(\\mathbf{x},y)$ are compiled into a dataset of examples $\\{(\\mathbf{x},y)_i\\}_{i=1}^N$, notice that $i$ here indexes examples not words in a text. Importantly, the sequence of words in each example are not guaranteed to be the same length. This presents a problem that prevents the straightforward application of standard supervised machine learning algorithms to text classification and motivates the use of a special class of models called *sequence models* (not considered here). The workaround we use in this article is much simpler and involves summarizing the text by a single vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing text as a sequence of word embeddings\n",
    "\n",
    "Since word embeddings are a good way to represent words, using them to represent each word in a text is also a good way to represent text. This is straightforward, each word in a text is replaced by its corresponding word embedding $w_1, w_2, ... w_n$. As a technical point, words not present in the vocabulary are replaced by the embedding for the \"UNK\" token. The sequence of word embeddings is then passed to a machine learning classifier (in our case, always a neural network). This transformation is standard in NLP as there are many (empirically observed) advantages to using word embeddings over one-hot encodings for words. \n",
    "\n",
    "One may wonder at this point where the word embeddings come from. We saw in the first article of this series that word embeddings can be trained in an unsupervised (no labels needed) fashion on a corpus. We could do the same on our dataset of examples $\\{(\\mathbf{x},y)_i\\}_{i=1}^N$ by training the word embeddings on the data $\\mathbf{x}$ and ignoring the labels $y$. However, this is not a good idea because we know from experience that learning good word embeddings requires a large dataset, usually millions of words long. Instead, we will leverage the concept of \"unsupervised pretraining\" by using word embeddings (pre)trained on a large corpus. Fortunately, there are many publically available pretrained word embeddings such as [word2vec](https://code.google.com/archive/p/word2vec/), [GloVe](https://github.com/stanfordnlp/GloVe), or [ELMO](https://allennlp.org/elmo). It is also possible to finetune a pretrained embedding for a specific task, but this is usually not a good idea unless the dataset for the task is large.\n",
    "\n",
    "![Example sentence embedding](./images/sent_embed1.png)\n",
    "\n",
    "In this figure, we can see a sentence represented as a sequence of word vectors. As mentioned before, all the sentences in a dataset may not be the same length, posing a problem for simple machine learing algorithms that require fixed size inputs. One simple way to deal with this issue is called the \"bag of words\". As shown in the figure, under the bag of words modeling assumption, the words in a sentence can be combined (by averaging or addition) into a single vector. This assumption ignores the order of the words in the sentence as there is no way to preserve the order of the words after they are thrown into the bag.\n",
    "\n",
    "<img src=\"./images/bag_of_words.jpg\" style=\"height: 200px;\">\n",
    "<center>A bag of words art display at Carnegie Mellon University.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple model setup\n",
    "\n",
    "As is standard in machine learning, we define a function $f(x;\\theta)$ (with input $x$ and parameters $\\theta$) that correctly matches input data $x$ with the labels $y$ of our dataset. In this case, the function $f(\\cdot;\\theta)$ is defined as a feedforward neural network, also known as multi-layer perceptron (MLP), that accepts as input a column vector $x \\in \\mathbb{R}^{d \\times 1}$ and produces as output a probability distribtion over predicted labels $p \\in \\mathbb{R}^{C \\times 1}$ (note that for C classes, $p$ is a vector that satisfies the conditions of a probability distribution, ie. (i) $\\sum_{c=1}^C{p_c} = 1$ and (ii) $p_c \\geq 0$ $\\forall c=1,2...C$).\n",
    "\n",
    "We will show how this is done for a neural network with two hidden layers of size $k_1$ and $k_2$. This network has three weight matrices which we will call $W^1, W^2,$ and $W^3$ and a [pointwise nonlinear activation function](https://en.wikipedia.org/wiki/Activation_function) $\\sigma(\\cdot) = ReLU(\\cdot)$. The first hidden layer $h^1$ is directly a function of the input $x$ and is defined as:\n",
    "$$ h^1 = \\sigma(W^1 x) $$\n",
    "Since $h^1$ is defined as a $k_1$-dimensional column vector, this means that $W^1$ is a $k_1 \\times d$ matrix. The second hidden layer $h^2$ is a function of the first hidden layer and defined as:\n",
    "$$ h^2 = \\sigma(W^2 h^1) $$\n",
    "Since $h^2$ is defined as a $k_2$-dimensional column vector, this means that $W^2$ is a $k_2 \\times k_1$ matrix. The output vector $p$ is defined as:\n",
    "$$ p = softmax(W^3 h^2) $$\n",
    "Thus, $W^3$ is a $ C \\times k_2$ matrix and $softmax(\\cdot)$ is a function that converts any vector into a probability distribution ([read about softmax here](https://en.wikipedia.org/wiki/Softmax_function)). We introduce the variables $h^1$ and $h^2$ for clarity, the entire neural network can be written as:\n",
    "$$ f(x;\\theta) := softmax(W^3 \\sigma(W^2 \\sigma(W^1 x))) $$\n",
    "Where the parameters of the neural network are entirely summarized as $\\theta = \\{W^1, W^2, W^3 \\}$. Note that for notational simplicity we are ignoring bias terms that are used in practice. The illustration below shows the diagram of this neural network.\n",
    "\n",
    "<img src=\"./images/neural_network1.png\" style=\"height:300px;\">\n",
    "\n",
    "We want to find the settings of our model parameters $\\theta$ that minimizes the models error in predicting the labels of the dataset with the expectation that such a parameter setting will be useful for data the model has never seen before (generalization). There exist many possible choices of error/loss functions we can choose to minimize, such as mean squared error or hinge loss. Let us focus on one particular error function called cross entropy error (CEE). CEE measures the distance between the probability vector associated with a true label $y$ (the one hot vector $t$) and the probability vector over labels outputted from the model ($p$ from above). The two vectors $t$ and $p$ are both $C \\times 1$ column vectors and satisfy both the properties of a probability distribution. CEE is defined as:\n",
    "$$ CEE(y,p) = -\\sum_{c=1}^C{t_c \\log{p_c}} $$\n",
    "\n",
    "By observing that $t$ is a one hot vector where $t_{y} = 1$ and zero everywhere else if $y$ is the true label, we can rewrite CEE as:\n",
    "$$ CEE(y,p) = -\\log{p_{y}} $$\n",
    "\n",
    "Now that we have defined a loss function and since we were careful to define the model with only differentiable operations, we can solve the following minimization problem on a dataset of labeled examples $\\{(x,y)_i\\}_{i=1}^N$ using gradient descent:\n",
    "$$ \\min_\\theta{ \\frac{1}{N} \\sum_{i=1}^N{CEE(y_i, f(x_i;\\theta)} } $$\n",
    "\n",
    "Before building our model we need to deal with converting a sequence of words into a fixed size input to our neural network. For the purposes of this demonstration, we will do this by combining the word embeddings for each text ($\\mathbf{x}$) into a single vector. The input to the neural network $x$ is then formed from the sequence of word embeddings $w_1, w_2, ... w_n$ as:\n",
    "$$ x = \\frac{1}{n}\\sum_{j=1}^n{w_j} $$\n",
    "Note that $x$ is the same size as a word embedding vector ($d \\times 1$) and is expected to summarize the text. Also note that the averaging operation loses all the word ordering information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Section\n",
    "\n",
    "In this section we will demonstrate how to code the simple model described above in Tensorflow and train it on a dataset for sentiment classification.\n",
    "\n",
    "The dataset we will be using for this demonstration is called the Stanford Sentiment Treebank (SST). We will use some utility functions written for SST provided as part of the [Stanford CS224D NLP course](https://cs224d.stanford.edu/syllabus.html), particularly code related to Question 4 from problem set 1. The required (and edited) [code]() is provided with this article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "The `StanfordSentiment` class gives us some utility functions to load the Stanford Sentiment Treebank dataset\n",
    "- `dataset.tokens()` - tokenizes the entire dataset\n",
    "- `dataset.getTrainSentences()` - returns a list of `(sentence, sentiment_label)` pairs in the training set\n",
    "- `dataset.getDevSentences()` - returns a list of `(sentence, sentiment_label)` pairs in the development set\n",
    "- `dataset.getTestSentences()` - returns a list of `(sentence, sentiment_label)` pairs in the test set\n",
    "\n",
    "The `sentence` in an example is a list of words (all lowercase). The `sentiment_label` in an example is the level of sentiment expressed in a sentence. There are five disjoint levels of sentiment: {very negative, negative, neutral, positive, very positive} encoded as {0,1,2,3,4}.\n",
    "\n",
    "Notice that there are three distinct datasets for a machine learning problem: training set, development set, and test set.\n",
    "- The training set is used to learn the parameters of the neural network using gradient descent\n",
    "- The development set is used to optimize hyperparameters (units in hidden layers, regularization, learning rate).\n",
    "- The test set is used to evaluate the model's performance\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens 19539\n",
      "Label: 3 - Sentence: the rock is destined to be the 21st century 's new `` conan '' and that he 's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "Label: 4 - Sentence: unlike the speedy wham-bam effect of most hollywood offerings , character development -- and more importantly , character empathy -- is at the heart of italian for beginners .\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from code_utils.data_utils import StanfordSentiment\n",
    "from random import shuffle\n",
    "\n",
    "#GPU session config\n",
    "sess_config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "\n",
    "# Load the dataset\n",
    "dataset = StanfordSentiment('./data/stanfordSentimentTreebank')\n",
    "tokens = dataset.tokens()\n",
    "print('Number of tokens',len(tokens))\n",
    "\n",
    "# Load the train set\n",
    "trainset = dataset.getTrainSentences()\n",
    "print('Label: %d - Sentence: %s' % (trainset[0][1], ' '.join(trainset[0][0])))\n",
    "\n",
    "# Load the dev set\n",
    "devset = dataset.getDevSentences()\n",
    "print('Label: %d - Sentence: %s' % (devset[10][1], ' '.join(devset[10][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained GLoVe embeddings\n",
    "\n",
    "As input to our model, we will use pretrained word embeddings called [Global Vectors (GloVe)](https://nlp.stanford.edu/projects/glove/) as input word representations to our model. GloVe embeddings are similar to the word2vec embeddings discussed in a previous [article](). The GloVe embeddings are trained in a slightly different way. Instead of maximizing the dot product of words that appear in the same context, GloVe attempts to learn embedding vectors such that the dot product of two words equals the logarithm of their **co-occurence** probability.\n",
    "\n",
    "The co-occurence probability represents how likely two words are to appear next to each other in natural language. This probability is calculated from a co-occurence matrix $C$, a $V \\times V$ ($V$ is the size of the vocabulary) matrix that counts the number of times two words appear next to each other in a natural language corpus. Specifically, each element $C_{ij}$ stores the number of times word $i$ co-occurs with word $j$. Once this matrix is computed, each count can be converted to a probability by dividing the count value by the total number of times the word appears in the corpus.\n",
    "\n",
    "The main idea behind matching dot products to the logarithms of co-ocurrence probabilities is that the co-occurence probabilities, or more specifically, the ratios of co-occurence probabilities encode some form of real meaning. If this is true, than the distance between vectors learned by GloVe embeddings represents the ratio of co-occurence probabilities, and by extension, encodes the real meanings of words. More details about the method for training word embeddings can be found in the [paper](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "\n",
    "Fortunately, the developers of GloVe embeddings have shared the pretrained embeddings trained on large corpuses. In this experiment, we will use 50-dimensional GloVe embeddings trained on a 6 billion token corpus (Wikipedia+Gigaword) with a 400,000 word vocabulary available [here](http://nlp.stanford.edu/data/glove.6B.zip). \n",
    "\n",
    "We will use the 50,000 most common words in the vocabulary for our sentiment classification experiment and replace words not in the vocabulary with the special \"UNK\" token. Also, we will introduce a specially \"PAD\" token with a 50-dimensional zero embedding vector in order to pad the sentences in the texts to the same length. This is a practical requirement for feeding the inputs to the model. \n",
    "\n",
    "Lastly, we create two python dicts (`word2id` and `id2word`) to make converting between words and word ids easily. Note that word ids are arbitrary, in the code we assign lower ids to more common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: profitability | ID: 10839 | Embedding Norm: 5.302562\n",
      "Word: rogers | ID: 5640 | Embedding Norm: 3.613782\n",
      "Word: 920 | ID: 37801 | Embedding Norm: 4.058931\n",
      "Word: 0-5 | ID: 38299 | Embedding Norm: 4.706855\n",
      "Word: scribes | ID: 45848 | Embedding Norm: 3.706744\n"
     ]
    }
   ],
   "source": [
    "# I have it saved in ./data/glove.6B.50d.txt\n",
    "embed_file = './data/glove.6B.50d.txt'\n",
    "vocab_size = 50000 # up to 400k\n",
    "embed_dim = 50\n",
    "\n",
    "embedding_mat = np.zeros((vocab_size,embed_dim), dtype=np.float32)\n",
    "word2id = {'PAD': 0,'UNK': 1}\n",
    "id2word = {0: 'PAD', 1: 'UNK'}\n",
    "\n",
    "# make UNK embedding a random vector\n",
    "embedding_mat[1,:] = np.random.rand((embed_dim))\n",
    "# get the rest of the embeddings we want from the file, we will get the \"vocab_size\" most common words.\n",
    "with open(embed_file,'r') as efile:\n",
    "    for index,line in enumerate(efile.readlines()):\n",
    "        splitted = line.split()\n",
    "        word2id[splitted[0]] = index+2\n",
    "        id2word[index+2] = splitted[0]\n",
    "        embedding_mat[index+2] = np.asarray(splitted[1:])\n",
    "        if index+2 == vocab_size-1: break\n",
    "\n",
    "#print a few random word and the norm of its embedding vector\n",
    "for _ in range(5):\n",
    "    r = np.random.randint(0,vocab_size)\n",
    "    print('Word: %s | ID: %d | Embedding Norm: %f' %(id2word[r], word2id[id2word[r]], np.linalg.norm(embedding_mat[r,:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper and batching functions\n",
    "\n",
    "Three functions are defined for loading data:\n",
    "- `word_lookup` - Looks up the id of a word in the `word2id` dictionary, if the word is not found (not in our vocabulary) then returns the id for \"UNK\".\n",
    "- `zero_pad` - Accepts as input a list of lists of word ids representing a batch of sentences. Uses zero padding to make all sentences in the batch the same length (`max_len`).\n",
    "- `get_batch` - Returns a list of size `batch_size` pairs-`(data,label)` from a given `dataset` and index in the dataset (`data_ind`). For training, it assumes the dataset is a list of example pairs that has already been shuffled (this is important for training neural networks). This function zero pads the sentences so that all sentences in the batch are the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 [1898, 1193, 21, 9, 1, 61, 2713, 7, 6745, 3, 36, 2, 93, 875, 83, 771, 6745, 16, 16305, 2, 1223, 5, 2997, 133, 22, 4]\n",
      "26 [436, 9, 1, 36172, 22627, 19, 9, 3257, 5, 281, 10165, 5856, 6510, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "26 [36, 1, 11, 35214, 39681, 909, 22, 1916, 1905, 380, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "26 [2, 1007, 11, 2221, 16, 593, 3464, 1, 7, 1, 3, 1782, 361, 17696, 57, 48, 89, 248, 1517, 49, 2516, 6, 8516, 9, 7301, 4]\n",
      "26 [262, 9, 221, 666, 5, 5048, 9, 81, 7, 243, 3, 7, 5, 2823, 9, 12318, 1397, 11, 525, 4, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def word_lookup(word, word2id):\n",
    "    if word in word2id:\n",
    "        return word2id[word]\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def zero_pad(sent_batch):\n",
    "    max_len = max([len(sent) for sent in sent_batch])\n",
    "    for sent in sent_batch:\n",
    "        sent.extend((max_len-len(sent))*[0])\n",
    "    return sent_batch\n",
    "\n",
    "def get_batch(dset, data_ind, batch_size):\n",
    "    nExamples = len(dset)\n",
    "    data = list()\n",
    "    labels = list()\n",
    "    if data_ind+batch_size <= nExamples:\n",
    "        indices = range(data_ind,data_ind+batch_size)\n",
    "    else: # roll over\n",
    "        indices = list(range(data_ind,nExamples))\n",
    "        indices.extend(list(range(batch_size-len(indices))))\n",
    "    for ind in indices:\n",
    "        sent, label = dset[ind]\n",
    "        data.append([word_lookup(word, word2id) for word in sent])\n",
    "        labels.append(label)\n",
    "    return zero_pad(data),labels\n",
    "\n",
    "# print an example batch\n",
    "ex_batch, labels = get_batch(trainset, len(trainset)-10, 5)\n",
    "for ex in ex_batch:\n",
    "    print(len(ex), ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the neural network for sentiment classification\n",
    "\n",
    "In the following section, we will show how to create the tensorflow graph for the simple two layer neural network described above. We define a function `build_model` that creates a tensorflow graph representing the model and returns the graph and the tensorflow operations (ops) we are interested in. `build_model` accepts the following input:\n",
    "- `l1_hidden_units` - number of hidden units in the first hidden layer of the network.\n",
    "- `l1_hidden_units` - number of hidden units in the second hidden layer of the network.\n",
    "- `num_classes` - number of possible classes. This defines the size of the output softmax layer of the network. In our case, this is 5.\n",
    "- `reg_val` - regularization parameter for L2 regularization. L2 regularization is a method to constrain the weights of a neural network for better generalization. We will optimize this as value as a hyperparameter.\n",
    "- `learning_rate` - Learning rate of the SGD optimizer. We will fix this at 0.3, but changing it can improve performance significantly. Try playing with this value.\n",
    "\n",
    "Here is a more detailed breakdown of the code:\n",
    "- Section 1: Placeholders are defined for the input sentences and labels in a batch. The `None` in the shape argument allows different size tensors (such as different batch sizes or sentence lengths) across `run` calls on the graph. The `class_labels` placeholder is a vector of size `batch_size` sentiment labels (0,1,2,3,4). `labels` converts `class_labels` to a matrix of one-hot vectors of size `batch_size,num_classes`. The `inputs` placeholder is a matrix of word ids of size `batch_size,max_len` where `max_len` is the length of the largest sentence in the batch.\n",
    "- Section 2: Computes the 50-dimensional sentence embedding for each sentence in the batch. `tf.nn.embedding_lookup` looks up the word embeddings in the pretrained GloVe `embedding` matrix by the word ids. Then we average the word embeddings in a sentence to get the `sent_embed`. Notice that we are using `tf.count_nonzero` and `tf.reduce_sum` to implement our own average instead of using `tf.reduce_mean` as we need to account for (ignore) the padding vectors. Also note that the `trainable=False` argument is used to make sure we don't modify the pretrained embeddings during training.\n",
    "- Section 3: We define the weights and biases of the first hidden layer with L2 regularization and define the output of the hidden layer as `l1_hidden` with a ReLU activation function.\n",
    "- Section 4: We define the weights and biases of the second hidden layer with L2 regularization and define the output of the hidden layer as `l2_hidden` with a ReLU activation function.\n",
    "- Section 5: The weights of the softmax layer are defined and the `logits` and output probabilities are separtately defined.\n",
    "- Section 6: The loss function consists of the cross-entropy loss between the true labels and predictions and the regularization penalty applied to the weights. The accuracy and SGD optimizer are also defined here.\n",
    "\n",
    "We end this section with a quick sanity check on the shapes of intermediate layers and the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables initialized\n",
      "Batch size: 128\n",
      "Embedding matrix shape: (50000, 50)\n",
      "Sentence embedding shape: (128, 50)\n",
      "Hidden layer 1 shape: (128, 500)\n",
      "Hidden layer 2 shape: (128, 250)\n",
      "Output prob shape: (128, 5)\n",
      "Loss value: 1.8277633\n",
      "Acc value: 0.234375\n"
     ]
    }
   ],
   "source": [
    "def build_model(l1_hidden_units, l2_hidden_units, num_classes=5, reg_val=1e-4, learning_rate=0.3):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        # Section 1: Input data placeholders\n",
    "        with tf.name_scope('inputs'):\n",
    "            inputs = tf.placeholder(tf.int32, shape=[None, None]) # batch x sentence length placeholder of embeddings\n",
    "            class_labels = tf.placeholder(tf.int32, shape=[None]) # batch placeholder of sentiment labels\n",
    "            labels = tf.one_hot(indices=class_labels, depth=num_classes, axis=-1)\n",
    "        # Section 2: Sentence embeddings\n",
    "        with tf.name_scope('sentence_embedding'):\n",
    "            embeddings = tf.get_variable(\"glove_embeddings\", initializer=embedding_mat, trainable=False)\n",
    "            word_embeds = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "            word_counts = tf.count_nonzero(word_embeds, axis=1)\n",
    "            sent_embed = tf.reduce_sum(word_embeds, axis=1)/tf.cast(word_counts, dtype=tf.float32)\n",
    "\n",
    "        # Section 3: Layer 1 weights and hidden units\n",
    "        with tf.name_scope('layer1'):\n",
    "            l1_weights = tf.get_variable(\"l1_weights\", shape=[embed_dim, l1_hidden_units], \n",
    "                                         regularizer=tf.contrib.layers.l2_regularizer(reg_val))\n",
    "            l1_biases = tf.get_variable(\"l1_biases\", shape=[l1_hidden_units],\n",
    "                                       regularizer=tf.contrib.layers.l2_regularizer(reg_val))    \n",
    "            l1_hidden = tf.nn.relu(tf.nn.bias_add(tf.matmul(sent_embed, l1_weights), l1_biases))\n",
    "        # Section 4: Layer 2 weights and hidden units\n",
    "        with tf.name_scope('layer2'):\n",
    "            l2_weights = tf.get_variable(\"l2_weights\", shape=[l1_hidden_units, l2_hidden_units],\n",
    "                                        regularizer=tf.contrib.layers.l2_regularizer(reg_val))\n",
    "            l2_biases = tf.get_variable(\"l2_biases\", shape=[l2_hidden_units],\n",
    "                                       regularizer=tf.contrib.layers.l2_regularizer(reg_val))\n",
    "            l2_hidden = tf.nn.relu(tf.nn.bias_add(tf.matmul(l1_hidden, l2_weights), l2_biases))\n",
    "        # Section 5: Output layer\n",
    "        with tf.name_scope('output_layer'):\n",
    "            out_weights = tf.get_variable(\"out_weights\", shape=[l2_hidden_units,num_classes],\n",
    "                                         regularizer=tf.contrib.layers.l2_regularizer(reg_val))\n",
    "            logits = tf.matmul(l2_hidden, out_weights)\n",
    "            probs = tf.nn.softmax(logits)\n",
    "        # Section 6: Loss function and optimizer            \n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "        loss += tf.losses.get_regularization_loss()\n",
    "        acc, acc_op = tf.metrics.accuracy(class_labels, tf.argmax(logits, axis=-1))\n",
    "        with tf.name_scope('optimizer'):\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)        \n",
    "        \n",
    "        \n",
    "    return graph, inputs, class_labels, loss, optimizer, acc_op, embeddings, sent_embed, l1_hidden, l2_hidden, probs\n",
    "# check that the model is correctly defined\n",
    "graph, inputs, class_labels, loss, optimizer, acc_op, embeddings, sent_embed, l1_hidden, l2_hidden, probs = build_model(500, 250, 5, 0.001, 0.5)\n",
    "with tf.Session(graph=graph, config=sess_config) as session:\n",
    "    sents, batch_labels = get_batch(trainset, 0, batch_size=128)\n",
    "    # We must initialize all variables before we use them.\n",
    "    tf.local_variables_initializer().run()\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Variables initialized') \n",
    "    print('Batch size:', len(sents))\n",
    "    print('Embedding matrix shape:',embeddings.eval().shape)    \n",
    "    print('Sentence embedding shape:', sent_embed.eval(feed_dict={inputs: sents}).shape)\n",
    "    print('Hidden layer 1 shape:', l1_hidden.eval(feed_dict={inputs: sents}).shape)\n",
    "    print('Hidden layer 2 shape:', l2_hidden.eval(feed_dict={inputs: sents}).shape)\n",
    "    print('Output prob shape:', probs.eval(feed_dict={inputs: sents}).shape)\n",
    "    print('Loss value:', loss.eval(feed_dict={inputs: sents, class_labels: batch_labels}))\n",
    "    print('Acc value:', acc_op.eval(feed_dict={inputs: sents, class_labels: batch_labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network and optimize regularization hyperparameter\n",
    "\n",
    "In this section, we train the parameters (weights and biases) of the network on the training dataset and attempt to optimize the regularization (hyper) parameter on the development set by trying different settings in a grid search. There are many other hyperparameters that can be optimized, such as hidden layer sizes, batch size, learning rate, regularization type, and initialization types, but we will not carry that out here. As an aside, an alternative to grid search is randomly trying different parameters as proposed by this [paper](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf) by Bergestra and Bengio.\n",
    "\n",
    "In the next code block, we visualize the results of the hyperparameter tuning.\n",
    "\n",
    "The code below has two loops: the outer loop iterates over different hyperparameter settings and the inner loop trains the model for `num_steps` iterations. We initialize a new model in each iteration of the outer loop, train it for `num_steps` iterations, and save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(+) Training with parameters: h1_size 500 | h2_size 250 | reg 0.000001\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Step          0: Avg train loss: 1.598886 | Avg train acc: 0.250000 | Dev loss: 1.563836 | Dev acc: 0.264368\n",
      "Step       5000: Avg train loss: 1.369981 | Avg train acc: 0.372775 | Dev loss: 1.463804 | Dev acc: 0.404808\n",
      "Step      10000: Avg train loss: 1.094850 | Avg train acc: 0.437401 | Dev loss: 1.808279 | Dev acc: 0.476459\n",
      "Step      15000: Avg train loss: 0.601781 | Avg train acc: 0.525529 | Dev loss: 3.086536 | Dev acc: 0.576440\n",
      "Step      20000: Avg train loss: 0.528516 | Avg train acc: 0.610546 | Dev loss: 2.861547 | Dev acc: 0.637204\n",
      "Step      25000: Avg train loss: 0.460465 | Avg train acc: 0.658549 | Dev loss: 3.110251 | Dev acc: 0.678095\n",
      "Step      30000: Avg train loss: 0.050673 | Avg train acc: 0.705702 | Dev loss: 5.394896 | Dev acc: 0.730749\n",
      "Step      35000: Avg train loss: 0.017336 | Avg train acc: 0.750870 | Dev loss: 4.414595 | Dev acc: 0.768996\n",
      "Step      40000: Avg train loss: 0.013494 | Avg train acc: 0.783988 | Dev loss: 4.795858 | Dev acc: 0.797681\n",
      "Step      45000: Avg train loss: 0.011917 | Avg train acc: 0.809284 | Dev loss: 5.551985 | Dev acc: 0.819988\n",
      "Step      50000: Avg train loss: 0.010965 | Avg train acc: 0.829235 | Dev loss: 5.558675 | Dev acc: 0.837834\n",
      "Step      55000: Avg train loss: 0.010424 | Avg train acc: 0.845377 | Dev loss: 5.940015 | Dev acc: 0.852435\n",
      "Step      60000: Avg train loss: 0.009975 | Avg train acc: 0.858704 | Dev loss: 5.944080 | Dev acc: 0.864604\n",
      "(+) Training with parameters: h1_size 500 | h2_size 250 | reg 0.000010\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Step          0: Avg train loss: 1.611977 | Avg train acc: 0.140625 | Dev loss: 1.613324 | Dev acc: 0.235632\n",
      "Step       5000: Avg train loss: 1.375084 | Avg train acc: 0.372348 | Dev loss: 1.346395 | Dev acc: 0.404129\n",
      "Step      10000: Avg train loss: 1.113819 | Avg train acc: 0.435350 | Dev loss: 2.030391 | Dev acc: 0.472503\n",
      "Step      15000: Avg train loss: 0.692500 | Avg train acc: 0.518681 | Dev loss: 2.598731 | Dev acc: 0.563576\n",
      "Step      20000: Avg train loss: 0.661364 | Avg train acc: 0.586401 | Dev loss: 3.211400 | Dev acc: 0.616951\n",
      "Step      25000: Avg train loss: 0.751553 | Avg train acc: 0.634886 | Dev loss: 2.665062 | Dev acc: 0.641329\n",
      "Step      30000: Avg train loss: 0.487767 | Avg train acc: 0.660172 | Dev loss: 2.990439 | Dev acc: 0.675783\n",
      "Step      35000: Avg train loss: 0.274613 | Avg train acc: 0.691904 | Dev loss: 4.745149 | Dev acc: 0.712324\n",
      "Step      40000: Avg train loss: 0.082996 | Avg train acc: 0.731016 | Dev loss: 4.699051 | Dev acc: 0.748094\n",
      "Step      45000: Avg train loss: 0.075678 | Avg train acc: 0.762564 | Dev loss: 5.078387 | Dev acc: 0.775916\n",
      "Step      50000: Avg train loss: 0.072682 | Avg train acc: 0.787446 | Dev loss: 5.290070 | Dev acc: 0.798172\n",
      "Step      55000: Avg train loss: 0.070723 | Avg train acc: 0.807574 | Dev loss: 5.019685 | Dev acc: 0.816380\n",
      "Step      60000: Avg train loss: 0.069015 | Avg train acc: 0.824198 | Dev loss: 5.694504 | Dev acc: 0.831558\n",
      "(+) Training with parameters: h1_size 500 | h2_size 250 | reg 0.000030\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Step          0: Avg train loss: 1.608532 | Avg train acc: 0.281250 | Dev loss: 1.612288 | Dev acc: 0.264368\n",
      "Step       5000: Avg train loss: 1.381176 | Avg train acc: 0.372964 | Dev loss: 1.371343 | Dev acc: 0.403478\n",
      "Step      10000: Avg train loss: 1.130656 | Avg train acc: 0.434345 | Dev loss: 1.853245 | Dev acc: 0.471396\n",
      "Step      15000: Avg train loss: 0.710252 | Avg train acc: 0.518523 | Dev loss: 2.355646 | Dev acc: 0.564566\n",
      "Step      20000: Avg train loss: 0.691882 | Avg train acc: 0.597900 | Dev loss: 3.045654 | Dev acc: 0.622266\n",
      "Step      25000: Avg train loss: 0.490233 | Avg train acc: 0.647475 | Dev loss: 3.809673 | Dev acc: 0.673525\n",
      "Step      30000: Avg train loss: 0.166145 | Avg train acc: 0.702144 | Dev loss: 4.621103 | Dev acc: 0.727554\n",
      "Step      35000: Avg train loss: 0.145445 | Avg train acc: 0.747914 | Dev loss: 4.502470 | Dev acc: 0.766257\n",
      "Step      40000: Avg train loss: 0.137480 | Avg train acc: 0.781424 | Dev loss: 4.776052 | Dev acc: 0.795276\n",
      "Step      45000: Avg train loss: 0.131009 | Avg train acc: 0.807019 | Dev loss: 4.764387 | Dev acc: 0.817851\n",
      "Step      50000: Avg train loss: 0.125510 | Avg train acc: 0.827209 | Dev loss: 4.265505 | Dev acc: 0.835913\n",
      "Step      55000: Avg train loss: 0.120731 | Avg train acc: 0.843545 | Dev loss: 4.835217 | Dev acc: 0.850687\n",
      "Step      60000: Avg train loss: 0.116497 | Avg train acc: 0.857030 | Dev loss: 4.086280 | Dev acc: 0.862999\n",
      "(+) Training with parameters: h1_size 500 | h2_size 250 | reg 0.000050\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Step          0: Avg train loss: 1.611435 | Avg train acc: 0.234375 | Dev loss: 1.570906 | Dev acc: 0.278736\n",
      "Step       5000: Avg train loss: 1.384802 | Avg train acc: 0.372564 | Dev loss: 1.403693 | Dev acc: 0.403877\n",
      "Step      10000: Avg train loss: 1.148374 | Avg train acc: 0.434029 | Dev loss: 1.761869 | Dev acc: 0.469601\n",
      "Step      15000: Avg train loss: 0.736871 | Avg train acc: 0.514920 | Dev loss: 3.052366 | Dev acc: 0.562713\n",
      "Step      20000: Avg train loss: 0.634666 | Avg train acc: 0.600741 | Dev loss: 3.466988 | Dev acc: 0.628789\n",
      "Step      25000: Avg train loss: 0.445185 | Avg train acc: 0.654979 | Dev loss: 3.715171 | Dev acc: 0.685976\n",
      "Step      30000: Avg train loss: 0.203711 | Avg train acc: 0.713597 | Dev loss: 4.329200 | Dev acc: 0.738035\n",
      "Step      35000: Avg train loss: 0.184892 | Avg train acc: 0.757612 | Dev loss: 3.862605 | Dev acc: 0.775248\n",
      "Step      40000: Avg train loss: 0.173406 | Avg train acc: 0.789834 | Dev loss: 4.093005 | Dev acc: 0.803152\n",
      "Step      45000: Avg train loss: 0.630511 | Avg train acc: 0.810745 | Dev loss: 1.965106 | Dev acc: 0.804654\n",
      "Step      50000: Avg train loss: 0.842944 | Avg train acc: 0.798380 | Dev loss: 3.009526 | Dev acc: 0.798001\n",
      "Step      55000: Avg train loss: 0.399767 | Avg train acc: 0.803436 | Dev loss: 3.945493 | Dev acc: 0.811063\n",
      "Step      60000: Avg train loss: 0.226007 | Avg train acc: 0.819104 | Dev loss: 4.103585 | Dev acc: 0.826677\n",
      "(+) Training with parameters: h1_size 500 | h2_size 250 | reg 0.000080\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Step          0: Avg train loss: 1.615079 | Avg train acc: 0.226562 | Dev loss: 1.642613 | Dev acc: 0.229885\n",
      "Step       5000: Avg train loss: 1.396296 | Avg train acc: 0.372444 | Dev loss: 1.428052 | Dev acc: 0.402206\n",
      "Step      10000: Avg train loss: 1.187868 | Avg train acc: 0.429811 | Dev loss: 1.696527 | Dev acc: 0.462214\n",
      "Step      15000: Avg train loss: 0.834913 | Avg train acc: 0.503505 | Dev loss: 2.814315 | Dev acc: 0.547421\n",
      "Step      20000: Avg train loss: 0.656366 | Avg train acc: 0.584778 | Dev loss: 3.113086 | Dev acc: 0.619210\n",
      "Step      25000: Avg train loss: 0.762080 | Avg train acc: 0.645735 | Dev loss: 3.114931 | Dev acc: 0.659187\n",
      "Step      30000: Avg train loss: 0.796803 | Avg train acc: 0.674091 | Dev loss: 3.176690 | Dev acc: 0.685465\n",
      "Step      35000: Avg train loss: 0.448262 | Avg train acc: 0.704946 | Dev loss: 3.811663 | Dev acc: 0.725337\n",
      "Step      40000: Avg train loss: 0.309572 | Avg train acc: 0.743179 | Dev loss: 3.988767 | Dev acc: 0.759476\n",
      "Step      45000: Avg train loss: 0.277349 | Avg train acc: 0.773288 | Dev loss: 3.834909 | Dev acc: 0.786033\n",
      "Step      50000: Avg train loss: 1.387720 | Avg train acc: 0.782269 | Dev loss: 2.022149 | Dev acc: 0.769800\n",
      "Step      55000: Avg train loss: 1.201565 | Avg train acc: 0.763930 | Dev loss: 2.547580 | Dev acc: 0.761501\n",
      "Step      60000: Avg train loss: 0.826085 | Avg train acc: 0.764108 | Dev loss: 3.808235 | Dev acc: 0.768437\n",
      "(+) Training with parameters: h1_size 500 | h2_size 250 | reg 0.000100\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Step          0: Avg train loss: 1.640766 | Avg train acc: 0.187500 | Dev loss: 1.608690 | Dev acc: 0.227011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step       5000: Avg train loss: 1.404243 | Avg train acc: 0.371592 | Dev loss: 1.507652 | Dev acc: 0.400644\n",
      "Step      10000: Avg train loss: 1.212410 | Avg train acc: 0.426755 | Dev loss: 1.785128 | Dev acc: 0.457346\n",
      "Step      15000: Avg train loss: 0.879291 | Avg train acc: 0.497350 | Dev loss: 2.386524 | Dev acc: 0.540336\n",
      "Step      20000: Avg train loss: 0.615910 | Avg train acc: 0.583054 | Dev loss: 2.913209 | Dev acc: 0.621168\n",
      "Step      25000: Avg train loss: 0.355122 | Avg train acc: 0.657845 | Dev loss: 3.379833 | Dev acc: 0.692378\n",
      "Step      30000: Avg train loss: 1.377558 | Avg train acc: 0.686990 | Dev loss: 2.920263 | Dev acc: 0.684048\n",
      "Step      35000: Avg train loss: 0.931830 | Avg train acc: 0.693164 | Dev loss: 3.275627 | Dev acc: 0.703110\n",
      "Step      40000: Avg train loss: 1.204760 | Avg train acc: 0.704407 | Dev loss: 2.987030 | Dev acc: 0.705372\n",
      "Step      45000: Avg train loss: 0.838541 | Avg train acc: 0.714560 | Dev loss: 3.529988 | Dev acc: 0.723223\n",
      "Step      50000: Avg train loss: 0.694309 | Avg train acc: 0.734542 | Dev loss: 1.938117 | Dev acc: 0.744595\n",
      "Step      55000: Avg train loss: 1.364821 | Avg train acc: 0.735164 | Dev loss: 2.930382 | Dev acc: 0.732857\n",
      "Step      60000: Avg train loss: 0.796015 | Avg train acc: 0.737770 | Dev loss: 3.512829 | Dev acc: 0.744380\n"
     ]
    }
   ],
   "source": [
    "num_steps = 60001\n",
    "batch_size = 128\n",
    "verbose = True\n",
    "\n",
    "def evaluate_model(dset, session, loss_op, acc_op):\n",
    "    batch_inputs, batch_labels = get_batch(dset, 0, len(dset))\n",
    "    feed_dict = {inputs: batch_inputs, class_labels: batch_labels}\n",
    "    loss, acc = session.run([loss_op, acc_op], feed_dict=feed_dict)\n",
    "    return loss, acc\n",
    "\n",
    "param_list = [(500,250,0.000001), (500,250,0.00001), (500,250,0.00003), (500,250,0.00005), (500,250,0.00008), (500,250,0.0001)]\n",
    "results = list()\n",
    "\n",
    "for h1_size, h2_size, reg in param_list:\n",
    "    print('(+) Training with parameters: h1_size %d | h2_size %d | reg %f' % (h1_size, h2_size, reg))\n",
    "    print('-'.join(50*['-']))\n",
    "    shuffle(trainset)\n",
    "    graph, inputs, class_labels, loss, optimizer, acc, embeddings, sent_embed, l1_hidden, l2_hidden, probs = build_model(h1_size, h2_size, num_classes=5, reg_val=reg)\n",
    "    model_path = 'models/model_%d_%d_%f.ckpt' % (h1_size, h2_size, reg)\n",
    "    with graph.as_default():\n",
    "        saver = tf.train.Saver() # saves model parameters\n",
    "    with tf.Session(graph=graph, config=sess_config) as session:\n",
    "        tf.local_variables_initializer().run()\n",
    "        tf.global_variables_initializer().run()\n",
    "        average_loss = 0\n",
    "        average_acc = 0\n",
    "        history = []\n",
    "        data_index = 0\n",
    "        for step in range(num_steps):\n",
    "            batch_inputs, batch_labels = get_batch(trainset, data_index, batch_size)\n",
    "            feed_dict = {inputs: batch_inputs, class_labels: batch_labels}\n",
    "            _, loss_val, acc_val = session.run([optimizer, loss, acc], feed_dict=feed_dict)\n",
    "            \n",
    "            average_loss += loss_val\n",
    "            average_acc += acc_val\n",
    "            if step % 5000 == 0 and verbose:\n",
    "                if step > 0:\n",
    "                    average_loss /= 5000\n",
    "                    average_acc /= 5000\n",
    "                # calculate loss on subset of development set\n",
    "                shuffle(devset)\n",
    "                dev_loss, dev_acc = evaluate_model(devset[0:len(devset)//5], session, loss, acc)\n",
    "                # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                print('Step %10d: Avg train loss: %f | Avg train acc: %f | Dev loss: %f | Dev acc: %f' \n",
    "                      % (step, average_loss, average_acc, dev_loss, dev_acc))\n",
    "                history.append({'train_loss': average_loss, 'train_acc': average_acc, 'dev_loss': dev_loss, 'dev_acc': dev_acc})\n",
    "                average_loss = 0\n",
    "                average_acc = 0\n",
    "\n",
    "            data_index += batch_size\n",
    "            if data_index >= len(trainset):\n",
    "                data_index = 0\n",
    "                shuffle(trainset)\n",
    "        \n",
    "        # after we are done training, evaluate train and dev accuracy and save model parameters\n",
    "        save_path = saver.save(session, model_path)\n",
    "        train_loss, train_acc = evaluate_model(trainset, session, loss, acc)\n",
    "        dev_loss, dev_acc = evaluate_model(devset, session, loss, acc)\n",
    "        results.append({'h1_size': h1_size,\n",
    "                       'h2_size': h2_size,\n",
    "                       'reg': reg,\n",
    "                       'train_loss': train_loss,\n",
    "                        'train_acc': train_acc,\n",
    "                       'dev_loss': dev_loss,\n",
    "                       'dev_acc': dev_acc,\n",
    "                       'history': history,\n",
    "                       'save_path': save_path})     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RESULTS SUMMARY ---\n",
      "*h1_size : 500.000000 | h2_size : 250.000000 | reg : 0.000001 | train_loss : 0.009266 | train_acc : 0.864753 | dev_loss : 5.854029 | dev_acc : 0.864676 | \n",
      " h1_size : 500.000000 | h2_size : 250.000000 | reg : 0.000010 | train_loss : 0.068493 | train_acc : 0.831744 | dev_loss : 5.503753 | dev_acc : 0.831672 | \n",
      " h1_size : 500.000000 | h2_size : 250.000000 | reg : 0.000030 | train_loss : 0.114955 | train_acc : 0.863151 | dev_loss : 4.288242 | dev_acc : 0.863072 | \n",
      " h1_size : 500.000000 | h2_size : 250.000000 | reg : 0.000050 | train_loss : 0.213827 | train_acc : 0.826868 | dev_loss : 4.170769 | dev_acc : 0.826796 | \n",
      " h1_size : 500.000000 | h2_size : 250.000000 | reg : 0.000080 | train_loss : 0.619775 | train_acc : 0.768621 | dev_loss : 3.463444 | dev_acc : 0.768559 | \n",
      " h1_size : 500.000000 | h2_size : 250.000000 | reg : 0.000100 | train_loss : 0.599380 | train_acc : 0.744619 | dev_loss : 3.631648 | dev_acc : 0.744557 | \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "reg_param = [res['reg'] for res in results]\n",
    "dev_acc = [res['dev_acc'] for res in results]\n",
    "train_acc = [res['train_acc'] for res in results]\n",
    "\n",
    "best_index = int(np.argmax(dev_acc))\n",
    "\n",
    "# print summary of results\n",
    "print('---- RESULTS SUMMARY ---')\n",
    "for i,res in enumerate(results):\n",
    "    res_str = ' '\n",
    "    if i == best_index: res_str='*'\n",
    "    for key, val in res.items():\n",
    "        if key in ['history', 'save_path']: continue\n",
    "        res_str+='%s : %f | ' % (key, val)\n",
    "    print(res_str)    \n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.semilogx(reg_param, train_acc, label='train')\n",
    "plt.semilogx(reg_param, dev_acc, label='dev')\n",
    "plt.xlabel('l2-regularization')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('regularization parameter optimization')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Summary\n",
    "\n",
    "We can clearly see that setting for the regularization parameter that is the best choice for this network. Clearly we can fine tune this and other hyperparameter values further, but even this simple tuning step results in about ~80% accuracy on the development set. The next step would be to select the best hyperparameter values and then evaluate the performance of the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/model_500_250_0.000001.ckpt\n",
      "Model restored. Evaluating performance on test data:\n",
      "Best Model: Test loss=6.134393 | Test accuracy=0.312217\n"
     ]
    }
   ],
   "source": [
    "best_model = results[best_index]['save_path']\n",
    "# rebuild the model\n",
    "graph, inputs, class_labels, loss, optimizer, acc, embeddings, sent_embed, l1_hidden, l2_hidden, probs = build_model(results[best_index]['h1_size'],\n",
    "                                                                                                                     results[best_index]['h2_size'],\n",
    "                                                                                                                     num_classes=5, \n",
    "                                                                                                                   reg_val=results[best_index]['reg'])\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver() # saves model parameters\n",
    "# load the test set\n",
    "testset = dataset.getTestSentences()\n",
    "\n",
    "with tf.Session(graph=graph, config=sess_config) as session:\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(session, best_model)\n",
    "    session.run(tf.local_variables_initializer())\n",
    "    print(\"Model restored. Evaluating performance on test data:\")\n",
    "    test_loss, test_acc = evaluate_model(testset, session, loss, acc)\n",
    "    print(\"Best Model: Test loss=%f | Test accuracy=%f\" % (test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this article we learned how to set up a simple text classification problem, specifically sentiment classification, for a machine learning model. We leveraged pretrained word embeddings to encode the words in a sentence and constructed very simple sentence embeddings by averaging word embeddings. The sentence embeddings were used as input to a two layer, regularized neural network that could predict the sentiment expressed in a sentence as one of five sentiment levels. We acheived around 82% accuracy on the development set using this simple model. \n",
    "\n",
    "Aside from further optimizing hyperparameters, we can improve our model by coming up with a better solution to creating sentence embeddings. Averaging word embeddings is a simple way to create a fixed size representation for our model, but notice that all word ordering and context information is lost after averaging. One way to preserve these kinds of properties is using a **sequence model** that can create a fixed size representation for a sentence and encode word order. We will discuss one family of sequence models called Recurrent Neural Networks in the next article."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_py3",
   "language": "python",
   "name": "tf_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
